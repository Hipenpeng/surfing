{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Background\n",
    "### 1.1 Scenario\n",
    "A group of surfers want to start a business that connects tourists with a 24 hour surfing experience on north or south coasts, involving over night accommodation and transport to a local surf spot. In order to deliver best experience within a fixed cost, they need to balance distance and traffic with weather and wave height. They want to know the best period of the year to run this service, which areas should be used for the overnight accommodation, and whether they can get near realtime data to allow them to decide which beach they are travelling to before  5:00am (their planned pickup time from the hotel).\n",
    "\n",
    "### 1.2  Interpretation\n",
    "The first problem for me is what the north or south coasts are. It very hard to tell, it depends on where you are and what the range is. In this notebook, I consider Brisbane as a centre because Brisbane has the most population in Queensland. So, the north coast should be Sunshine Coast and the south coast should be Gold coast. \n",
    "\n",
    "For this business, <b>the first important thing</b> is to figure out who our customers are and is it worthy to run this business. <b>Then</b>, I should consider which months of a year are the surfing period. After setting the surfing season, <b>the next step</b> is to get surfing spots forecast data of weather and the wave condition and find the best surfing spot for our customers. <b>when it comes to the accommodation</b>, the basic idea is the location should be in Gold Coast and Sunshine Coast because it is close to the surfing spots and customers do not want to have a long trip before they arrive their surfing spots. <b>I will not consider the traffic condition</b> in this Jupyter notebook because the traffic condition of these two cities is great except for the peak period. Just avoid the peak period in advance and follow the Google map guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Who is our customers?\n",
    "Before we start our business, we should figure out how big the market is, what kind of people we want to serve.\n",
    "\n",
    "Currently, there are more than 35 million surfers worldwide and the estimated global surf industry spend is more than $10 billion a year. Australia is renowned as one of the world's premier surfing destinations, so it also attracts a lot of surfers from the world. At the same time, in Australia <b> alone one in every 20 people surf.</b> There are approximately 2.5 million recreational surfers in Australia, 420,000 annual surf participants. We can feel this passion from the post on twitter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Feeling the surfing passion on twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the data from the Twitter API and analyse how people think about surfing in Australia "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import tweepy           # To access and consume Twitter's API\n",
    "import pandas as pd     # To handle data\n",
    "import numpy as np      # For number computing\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "\n",
    "# Twitter App access keys\n",
    "\n",
    "# Consume:\n",
    "CONSUMER_KEY    = 'kqboNqw0Wid2C2Hq5SjkMgAnL'\n",
    "CONSUMER_SECRET = '6f9lnniVRW0fAO9qnmWeVrueJvzbMW6omWcIEssyWfHtLBVgqx'\n",
    "\n",
    "# Access:\n",
    "ACCESS_TOKEN  = '1109325911944421377-zKKyOUVCvZQszex0gFBfjTNgoCtBey'\n",
    "ACCESS_SECRET = 'NOnVAYOmDpa1WRD3qfgx1rYH2CjCdKecGJ6ILXUW9goyI'\n",
    "\n",
    "# API's setup:\n",
    "def connectToTwitterAPI():\n",
    "    \"\"\"\n",
    "    Utility function to setup the Twitter's API\n",
    "    with access keys.\n",
    "    \"\"\"\n",
    "    # Authentication and access using keys\n",
    "    auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "    auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)\n",
    "\n",
    "    # Return API with authentication\n",
    "    api = tweepy.API(auth)\n",
    "    return api\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an extractor object\n",
    "extractor = connectToTwitterAPI()\n",
    "\n",
    "#Search Australia on Twitter\n",
    "# Specify search criteria and extract tweets into a list\n",
    "tweets = extractor.search(q=\"#surfing Australia\", lang = \"en\", count=1000)\n",
    "\n",
    "# Print the total number of extracted tweets\n",
    "print(\"Number of tweets extracted: {}.\\n\".format(len(tweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pandas dataframe\n",
    "data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])\n",
    "\n",
    "# Add relavant data from each tweet\n",
    "data['len']  = np.array([len(tweet.text) for tweet in tweets]) #textual content legnth\n",
    "data['ID']   = np.array([tweet.id for tweet in tweets])\n",
    "data['Date'] = np.array([tweet.created_at for tweet in tweets])\n",
    "data['Source'] = np.array([tweet.source for tweet in tweets])\n",
    "data['Likes']  = np.array([tweet.favorite_count for tweet in tweets]) #likes counts\n",
    "data['RTs']    = np.array([tweet.retweet_count for tweet in tweets]) #retweets count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cleanTweet(tweet):\n",
    "    '''\n",
    "    Utility function to clean the text in a tweet by removing \n",
    "    links and special characters using regex.\n",
    "    '''\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "\n",
    "def analyseSentiment(tweet):\n",
    "    '''\n",
    "    Utility function to classify the polarity of a tweet\n",
    "    using textblob.\n",
    "    '''\n",
    "    analysis = TextBlob(cleanTweet(tweet))\n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return 1\n",
    "    elif analysis.sentiment.polarity == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sentiment for each tweet and add the result into a new column\n",
    "data['Sentiment'] = np.array([ analyseSentiment(tweet) for tweet in data['Tweets'] ])\n",
    "\n",
    "# Display the first 10 elements of the dataframe\n",
    "display(data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct lists with classified tweets\n",
    "\n",
    "positiveTweets = [ tweet for index, tweet in enumerate(data['Tweets']) if data['Sentiment'][index] > 0]\n",
    "neutralTweets = [ tweet for index, tweet in enumerate(data['Tweets']) if data['Sentiment'][index] == 0]\n",
    "negativeTweets = [ tweet for index, tweet in enumerate(data['Tweets']) if data['Sentiment'][index] < 0]\n",
    "\n",
    "# Calculate percentages\n",
    "\n",
    "positivePercent = len(positiveTweets)*100/len(data['Tweets'])\n",
    "neutralPercent = len(neutralTweets)*100/len(data['Tweets'])\n",
    "negativePercent = len(negativeTweets)*100/len(data['Tweets'])\n",
    "\n",
    "# Print percentages\n",
    "\n",
    "print(\"Percentage of positive tweets: {}%\".format(positivePercent))\n",
    "print(\"Percentage of neutral tweets: {}%\".format(neutralPercent))\n",
    "print(\"Percentage de negative tweets: {}%\".format(negativePercent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "\n",
    "labels = ['Positive', 'Neutral', 'Negative']\n",
    "sizes = [positivePercent, neutralPercent, negativePercent]\n",
    "\n",
    "# Set different colors\n",
    "colors = ['green', 'grey', 'red']\n",
    "\n",
    "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can know the result of the pie chart above. Most people show a positive attitude to Australia surfing and only less 10% of people show a negative attitude. So, surfing business has a huge market in Australia and people love these sports so much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Figure out the market\n",
    "However our business mainly focuses on north and south coasts(Sunshine coast and Gold coast) in Queensland, that means we need to consider whether the population of cities near these two coasts can support our business or not. So, let's do it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install bs4\n",
    "#!pip install html5lib\n",
    "!pip install bs4\n",
    "!pip install html5lib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "# webscrapping\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import html5lib\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>We get population html from the wekipedia and clean the html data to get we want</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Population_list = [] # save the population from different citys\n",
    "# general function that receives a url, and returns an html page ready to be parsed\n",
    "def get_HTML(url):\n",
    "    response = urllib.request.urlopen(url)     # connect to server\n",
    "    html = response.read()                     # if access is allowed\n",
    "    return html                                # return html document of the given url\n",
    "Australia_City_Population_HTML = get_HTML('https://en.wikipedia.org/wiki/List_of_cities_in_Australia_by_population')\n",
    "\n",
    "soup = BeautifulSoup(Australia_City_Population_HTML, \"html.parser\")\n",
    "span_element = soup.find(text='Greater Capital City Statistical Areas/Significant Urban Areas by population')\n",
    "h2_element = span_element.parent\n",
    "table_element = h2_element.findNext('table') # a parent tag\n",
    "\n",
    "for tr_element in table_element.findAll('tr'): # find the city name and population\n",
    "    City_list=[]\n",
    "    i=0\n",
    "    for td_element in tr_element.findAll('td'):\n",
    "        \n",
    "        if i==1:\n",
    "            City_list.append(td_element.a.text)\n",
    "        if i == 3:\n",
    "            City_list.append(td_element.text.strip('\\n'))\n",
    "        i+=1\n",
    "    if City_list != []:   \n",
    "        Population_list.append(City_list)   \n",
    "\n",
    "Population_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preliminary data cleaning, the data is easy for us to read. However, the data of the population is a string and this data type is not ready for calculation. So, we need to clean further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#transfer the population data type from string to int\n",
    "for city_list in Population_list: \n",
    "    city_list[1]=city_list[1].strip('\\n')\n",
    "    city_list[1]=city_list[1].replace(',','')\n",
    "    city_list[1]=int(city_list[1])\n",
    "Population_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have found out the population of different cities in Australia. The next step is to find the population of cities near the Sunshine Coast and the Gold Coast. The main cities near the north coast and south coast are  \n",
    "\n",
    "                1.Brisbane\n",
    "                2.Gold Coast\n",
    "                3.Sunshine Coast\n",
    "                4.Toowoomba\n",
    "                5.Bundaberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Calculate the population of Brisbane, Gold Coast, Sunshine Coast, Toowoomba, Bundaberg\n",
    "Near_cities=[\"Brisbane\",\"Gold Coast\",\"Sunshine Coast\",\"Toowoomba\",\"Bundaberg\"]\n",
    "total_population=0\n",
    "y=[]\n",
    "for i in Near_cities:\n",
    "    for city_list in Population_list:\n",
    "        if city_list[0]== i:\n",
    "            print ('The population of '+ city_list[0]+ ' is ' + str(city_list[1]) )\n",
    "            y.append(city_list[1])\n",
    "            total_population = total_population + city_list[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the poulation\n",
    "#Import the plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Setup the data\n",
    "\n",
    "x = [\"Brisbane\",\"Gold Coast\",\"Sunshine Coast\",\"Toowoomba\",\"Bundaberg\"]\n",
    "colours = ['red','green','pink','yellow','blue']\n",
    "#Plot the data\n",
    "plt.bar(x,y, color=colours)\n",
    "\n",
    "#Lable the chart\n",
    "plt.ylabel('population')\n",
    "plt.xlabel('city')\n",
    "plt.title('Pupulation of cities near Sunshine Coast and Gold Coast')\n",
    "\n",
    "print ('The total population of Brisbane, Gold Coast, Sunshine Coast, Toowoomba, Bundaberg is ' + str(total_population) )\n",
    "\n",
    "population_like_surfing = float(total_population)/20.0\n",
    "print('The population of liking surfing is about '+ str(int(population_like_surfing))+'(in Australia alone one in every 20 people surf)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too bad!!!\n",
    "\n",
    "We still have <b>184149</b> persons like surfing. It is worthy to run this business. Actually, we also have many tourists from Australia other places and other countries. The total number should be over <b>200000</b> every year. From the visualization, we also know Brisbane has the most population in these five cities. Although we can know this from the number, the visualization makes the difference become more impressive. \n",
    "\n",
    "So we know the capability of the market. But the number 200000 is still a huge number for our business because we just started and we don't have experience.\n",
    "\n",
    "Let's make the number smaller and find our target customer. This will make our business easier to be operated and it is good for the company which just formed.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### 2.2 Analysis on market trending and target customer\n",
    "\n",
    "Firstly, let's do the analysis on the graph below.\n",
    "\n",
    "Between 2010 and 2014, the number of Aussie women taking part in surfing rose from 218,000 to 258,000, an increase of almost 20%. The number of teenage girls aged 14-17 who surf regularly or occasionally grew from 31,000 to 50,000 over that time, while the number of 18-24-year-old women rose from 46,000 to 59,000. The sport is also experiencing a boom among women aged 50+, 58,000 of whom hit the surf last year, up from 40,000 in 2010 – a 45% increase in participation.\n",
    "\n",
    "66,000 Aussie men aged 18-24 surfed occasionally or regularly in 2010, that figure has since plummeted to 30,000: almost half the amount of women the same age who surf. Declining participation among teenage boys aged between 14 and 17 means that they too are now outnumbered by their female peers. In contrast, men aged 50+ are taking to the waves in ever-increasing numbers: up from 93,000 in 2010 to 169,000 in 2014.  \n",
    "\n",
    "After analysis, we know the two most important information. The first is more and more women are engaging the surfing now. The second one is the number of men and women aged 50+ who participate in this sport is increasing dramatically.\n",
    "\n",
    "It is very obvious that we should our business focus on <b>customers aged 50+ and women surfers</b>.\n",
    "\n",
    "#### Surfing participation in Australia: 2010 vs 2014\n",
    "\n",
    "<img src='Surfing-boys-girls-chart.jpg' >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. When and where to run the business?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should ask ourselves a question before we consider the time to run the business. When does our customer want to go surfing? The answer is very easy. When they are feeling hot. Some people start to feel hot when the <b>temperature</b> arrives around 30 °C. Then the next step is to find months the highest over the 30 °C. At the same time, the <b>water temperature</b> is also very important because it will hurt people when the temperature is less than 25 °C. Another important element need to be considered is the <b>wave height</b>.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's view the average weather condition based on month in southern Queensland\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "# Set variables for file and index column\n",
    "file = 'Avtemp.csv' #see above\n",
    "colname = 'Month' #open the csv and have a look\n",
    "\n",
    "# Read \n",
    "avtemp = pandas.read_csv(file, index_col= colname)\n",
    "print(avtemp.shape)\n",
    "avtemp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the average highest and lowest temperature in a year\n",
    "month=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\n",
    "avtemp_mon = avtemp.filter(month, axis=1)\n",
    "avtemp_mon\n",
    "avtemp_mon_maixum = avtemp_mon.loc['Highest']\n",
    "avtemp_mon_minium = avtemp_mon.loc['Lowest']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the data for highest\n",
    "avtemp_mon_maixum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take a look at the data for lowest\n",
    "avtemp_mon_minium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us see the temperature trending in the whole\n",
    "# Add labels and set colours\n",
    "plt.plot(avtemp_mon_minium,'g-',label='Lowest')\n",
    "plt.plot(avtemp_mon_maixum,'m-',label='Highest')\n",
    "\n",
    "# Create legend.\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Temperature')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily find temperature trending in the whole year through this visualization. <b>November, December, January, February and March</b> average highest temperatures are over 30 °C. From February the temperature starts to go down. June to August is winter and July is the coldest month in Queensland. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Water Temperature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is the data of sea water temperatur near gold coast and sunshine coast\n",
    "file1 = 'Average sea temp.csv' #see above\n",
    "colname = 'Sta' #open the csv and have a look\n",
    "\n",
    "# Read \n",
    "avtemp1 = pandas.read_csv(file1, index_col= colname)\n",
    "print(avtemp1.shape)\n",
    "avtemp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the average highest and lowest temperature in a year\n",
    "month=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\n",
    "avtemp_mon1 = avtemp1.filter(month, axis=1)\n",
    "avtemp_mon1\n",
    "avtemp_mon_max1 = avtemp_mon1.loc['Max C']\n",
    "avtemp_mon_min1 = avtemp_mon1.loc['Min C']\n",
    "avtemp_mon_avg1 = avtemp_mon1.loc['Avg C']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us see the water temperature trending in the whole\n",
    "# Add labels and set colours\n",
    "plt.plot(avtemp_mon_min1,'g-',label='Lowest')\n",
    "plt.plot(avtemp_mon_max1,'m-',label='Highest')\n",
    "plt.plot(avtemp_mon_avg1,'h-',label='Average')\n",
    "# Create legend.\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Temperature')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily find <b>November, December, January, February and March</b> average highest temperatures are over 30 °C though this visualization. At the same time, the water temperature is also ready for surfing. We also find the temperature and water temperature is very stable and we do not need to worry the temperature changes rapidly. In addition, there is a summer holiday for students in these five months and it also the tourist season. So, we should run our main business in these 5 months. When it comes to which day, it is up to weather condition and wave condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Wave Height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wave height should not over 1.8m because we need to consider our customer safety.\n",
    "\n",
    "Firstly, let's draw the wave height in the last two years of Gold Coast and Sunshine Coast and we maybe find some regular laws that very important.\n",
    "\n",
    "There are thousands of records in a year. We can not read them one by one and draw them on the paper. So, we need the technology tool to make it become possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the required libraries\n",
    "\n",
    "# Data Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from pandas.tools.plotting import autocorrelation_plot\n",
    "\n",
    "# Data Visualization \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset containing height of different years analyse the structure\n",
    "df = pd.read_csv( 'gold_coast_2018.csv' )\n",
    "print( df.head() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that can draw wave height in one year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "def drawHeight(filename):\n",
    "    with open(filename) as f:\n",
    "\n",
    "        Greader= csv.reader(f)\n",
    "        header_row=next(Greader)\n",
    "        Gold_Hs=[]\n",
    "        Gold_time=[]\n",
    "        for row in Greader:\n",
    "            high=float(row[1])\n",
    "            if high == -99.9:\n",
    "                    high=1\n",
    "            Gold_Hs.append(high)\n",
    "         \n",
    "            time=datetime.strptime(row[0],\"%m/%d/%Y %H:%M\")\n",
    "            Gold_time.append(time)\n",
    "#Let us draw the wave height trending in the whole\n",
    "# Add labels\n",
    "        fig = plt.figure(dpi=128,figsize=(10,6))\n",
    "        plt.plot(Gold_time,Gold_Hs,'g-',label='Lowest')\n",
    "\n",
    "# Create legend.\n",
    "\n",
    "        plt.xlabel('Month')\n",
    "        plt.ylabel('Significant Height')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Draw the Gold Coast wave height in 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "drawHeight('gold_coast_2018.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br></br></br>\n",
    "#### Draw the wave height of Gold Coast in 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "drawHeight('gold_coast_2017.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After Comparing the two graphs of two years, we found the wave height of December and January is stable and perfect for surfing. The wave height in other months is not stable, but there are still many days suitable for surfing during these months. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "</br>\n",
    "</br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Draw Sunshine Coast wave height in 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "drawHeight('sunshine_coast_2018.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br></br></br>\n",
    "#### Draw wave height of Sunshine Coast in 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "drawHeight('sunshine_coast_2017.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the wave in the Sunshine Coast was not stable.\n",
    "\n",
    "The visualization of the wave height makes it easier for us to find rules of the wave.\n",
    "\n",
    "In conclusion, it is hard to find the law of wave height because it depends on many elements like tide, wind, weather and other sea areas conditions etc. So, It is very important to get the forecast according to real-time data and make the decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Get The Forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get and use the forecast data to decide whether we run the business in the next few hours or not and choose which surfing spot is the best place. The data is from the webscpapping and use the BeautifulSoup to parse and analyse the html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# webscrapping\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# for regurlar expressions\n",
    "import re\n",
    "\n",
    "# image display\n",
    "from IPython.core.display import HTML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the surfing spot we want to search for\n",
    "def get_HTML_forecast( place ):\n",
    "    # specify a header (if this script returns a ConnectionError exception, just change the name in the header)\n",
    "    headers = {'User-Agent': 'Catarina\\'s_request'}\n",
    "\n",
    "    place = place.replace(\" \", \"%20\")\n",
    "    # get the HTML page that contains the results of our search for the specified product\n",
    "    # you need to combine the url to make quesries on TESCO together with the product that you are searching for\n",
    "    link = \"http://www.swellmap.com/surfing/queensland/\"+place+\"#tables\" \n",
    "\n",
    "    # connect to server. If the server returns a code different from 200, it means there was a connection error\n",
    "    # and it was not possible to connect to the server\n",
    "    response = requests.get( link, headers = headers )\n",
    "    if response.status_code != 200:\n",
    "        raise ConnectionError\n",
    "\n",
    "    # creates a parse tree that can be used to extract contents from HTML documents, which can be used for web scraping\n",
    "    soup = BeautifulSoup( response.content, 'html.parser')\n",
    "    return soup\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Input the surfing spots from the next list</b>\n",
    "\n",
    "Cold Coast:\n",
    "\n",
    "           1. south-stradbroke-island          \n",
    "           2. the-spit\n",
    "           3. narrowneck\n",
    "           4. burleigh-heads\n",
    "           5. currumbin-point\n",
    "           6. greenmount\n",
    "           \n",
    "Sunshine Coast:\n",
    "\n",
    "           1. coolum-beach          \n",
    "           2. pin-cushion-(maroochydore)\n",
    "           3. kawana\n",
    "           4. happys-(caloundra)\n",
    "           5. alexandria-bay-(noosa)\n",
    "           6. boiling-pot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SurfingSpot=\"coolum-beach\"  #input the surfing spot\n",
    "forecast_doc = get_HTML_forecast(SurfingSpot) #get the forecast of the surfing spot you input html  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_forecast_table( soup, indx ):  #get the forecast table\n",
    "    soup_ele = soup.findAll('table', {'class' : 'table table-striped table-condensed table-center' } )[indx]\n",
    "    return soup_ele\n",
    "\n",
    "forecast_elem= get_forecast_table(forecast_doc, 0) # Get the forecast table html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the forecast data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the required libraries\n",
    "\n",
    "# Data Manipulation\n",
    "import numpy as np\n",
    "\n",
    "#from pandas.tools.plotting import autocorrelation_plot\n",
    "\n",
    "# Data Visualization \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the time from the forecast table\n",
    "def get_time( forecast_elem  ):\n",
    "    table_bodys_time=[]  \n",
    "    table_bodys = forecast_elem.findAll(\"td\",{\"class\" : \"time\"})\n",
    "    for td in table_bodys:\n",
    "        table_bodys_time.append(td.text)\n",
    "    return table_bodys_time\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the rating information\n",
    "def get_rating( forecast_elem  ):  \n",
    "    table_bodys_rating=[] \n",
    "    table_bodys = forecast_elem.findAll(\"td\",{\"class\" : \"rating\"})\n",
    "    for td in table_bodys:\n",
    "        table_bodys_rating.append(int(td.text))\n",
    "    return table_bodys_rating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the weather information\n",
    "def get_summary( forecast_elem  ):\n",
    "    table_bodys_summary=[]  \n",
    "    table_bodys = forecast_elem.findAll(\"td\",{\"class\" : \"summary\"})\n",
    "    for td in table_bodys:\n",
    "        table_bodys_summary.append(td.img['title'])\n",
    "    return table_bodys_summary \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the swell information\n",
    "def get_swell( forecast_elem  ):\n",
    "    \n",
    "    table_bodys_swell=[]\n",
    "       \n",
    "    table_bodys = forecast_elem.findAll(\"td\",{\"class\" : \"hs_sw\"})\n",
    "    for td in table_bodys:\n",
    "        table_bodys_swell.append(float(td.text))\n",
    "    return table_bodys_swell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the setface height from the forecast\n",
    "def get_setface( forecast_elem  ):\n",
    "\n",
    "    table_bodys_Setface=[]\n",
    "    table_bodys = forecast_elem.findAll(\"td\",{\"class\" : \"wface\"})\n",
    "    for td in table_bodys:\n",
    "        table_bodys_Setface.append(float(td.text))\n",
    "    return table_bodys_Setface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the wave height from the forecast\n",
    "def get_wave( forecast_elem  ):\n",
    "\n",
    "    \n",
    "    table_bodys_wave=[]\n",
    "    table_bodys = forecast_elem.findAll(\"td\",{\"class\" : \"hs\"})\n",
    "    for td in table_bodys:\n",
    "        table_bodys_wave.append(float(td.text))\n",
    "    return table_bodys_wave\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the height of wave and print the weather condition\n",
    "def get_forecast( forecast_elem  ):\n",
    "    time=['4h','10h','16h','22h','28h','34h','40h','46h','52h','58h','64h','70h']\n",
    "    rating=get_rating( forecast_elem  )\n",
    "    summary=get_summary( forecast_elem  )\n",
    "    wave=get_wave( forecast_elem  )\n",
    "    set_face=get_setface( forecast_elem  )\n",
    "    swell=get_swell( forecast_elem  )  \n",
    "    \n",
    "    plt.plot(time[0:12],wave[0:12],'g-',label='Wave Height')\n",
    "    plt.plot(time[0:12],set_face[0:12],'m-',label='Set Face')\n",
    "    plt.plot(time[0:12],swell[0:12],'b-',label='Swell')\n",
    "    plt.legend(loc='upper right')    \n",
    "# Create legend.\n",
    "\n",
    "    plt.xlabel('time')\n",
    "        \n",
    "    plt.ylabel('Height  /m')\n",
    "    plt.title('Wave height of '+SurfingSpot+' in next 70 hours')\n",
    "    for i in range(12):    #Print the weather condition\n",
    "        print (\"The weather of \"+SurfingSpot +\" in next \"+time[i]+\" is \"+summary[i])\n",
    "        \n",
    "def draw_rating(forecast_elem):      \n",
    "    time=['4h','10h','16h','22h','28h','34h','40h','46h','52h','58h','64h','70h']\n",
    "    rating=get_rating( forecast_elem  )\n",
    "    plt.plot(time[0:12],rating[0:12],'p-',label='Surfing Rating')\n",
    "    plt.ylabel('Rating')\n",
    "    plt.xlabel('time')\n",
    "    \n",
    "    plt.title('Surfing rating of '+SurfingSpot+' trending in next 70 hours')\n",
    "get_forecast( forecast_elem  )\n",
    "###If you find there is no green line on the graph, the green one should have merged with the purple one!!! That means thet have the same value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, We know the different wave height trending from the visualize graph and the weather condition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw the rating trending in next 70 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_rating(forecast_elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rating between <b>7 and 10</b> indicates<b> good quality</b> surf, based on a long swell period, little or no wind, and swell height greater than a metre. If the wind is quite strong, then it will be an offshore wind.\n",
    "\n",
    "Rating between<b> 4 and 6</b> indicates<b> reasonable</b> surf conditions, although possibly affected by light-fair onshore winds, and a lower swell.\n",
    "\n",
    "Rating between <b>1 and 3</b> suggests <b>poor conditions</b> such as strong onshore winds, a low swell period and a small swell height, all of which can make surfing more difficult.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! Now, we know the weather condition, the wave height and the surfing rating. We can make the decision whether we run a business or not in the next few hours, how long we should finish and which surfing spot should we choose according to the graph above. The next question is where our customers live overnight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.How to choose the accommodation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the Tripadvisor website to find the best accommodation for us. We find there is always a best seller tag on the hotel which is the most popular in the region we search. Then we analyse the reasons why the hotel become the most popular and we found next elements:\n",
    "    \n",
    "        1. The price is intermediate and most people can afford it\n",
    "        2. The amenities and the services are good\n",
    "        3. The location is good. Customers can access to transportation and their destination easily. \n",
    "This is perfect for our business because we can provide our customer with great accommodation with good services and transportation in a fixed budget.\n",
    "\n",
    "<b>Customers make the best decison for us!!!</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the HTML page that contains the results of our search for the Gold Coast\n",
    "def get_HTML_GCHOTEL( ):\n",
    "    # specify a header (if this script returns a ConnectionError exception, just change the name in the header)\n",
    "    headers = {'User-Agent': 'Catarina\\'s_request'}\n",
    "\n",
    "    \n",
    "    \n",
    "    # you need to combine the url to make quesries on TESCO together with the product that you are searching for\n",
    "    link = \"https://www.tripadvisor.com.au/Hotels-g255337-Gold_Coast_Queensland-Hotels.html\"\n",
    "\n",
    "    # connect to server. If the server returns a code different from 200, it means there was a connection error\n",
    "    # and it was not possible to connect to the server\n",
    "    response = requests.get( link, headers = headers )\n",
    "    if response.status_code != 200:\n",
    "        raise ConnectionError\n",
    "\n",
    "    # creates a parse tree that can be used to extract contents from HTML documents, which can be used for web scraping\n",
    "    soup = BeautifulSoup( response.content, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "\n",
    "\n",
    "# get the HTML page that contains the hotel results of our search for the Sunbshine Coast\n",
    "def get_HTML_SSHOTEL( ):\n",
    "    # specify a header (if this script returns a ConnectionError exception, just change the name in the header)\n",
    "    headers = {'User-Agent': 'Catarina\\'s_request'}\n",
    "\n",
    "\n",
    "    link = \"https://www.tripadvisor.com.au/Hotels-g1132645-Sunshine_Coast_Queensland-Hotels.html\"\n",
    "\n",
    "    # connect to server. If the server returns a code different from 200, it means there was a connection error\n",
    "    # and it was not possible to connect to the server\n",
    "    response = requests.get( link, headers = headers )\n",
    "    if response.status_code != 200:\n",
    "        raise ConnectionError\n",
    "\n",
    "    # creates a parse tree that can be used to extract contents from HTML documents, which can be used for web scraping\n",
    "    soup = BeautifulSoup( response.content, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def get_best_seller_soup( soup ):\n",
    "    groceries_soup = soup.find(text='Best Seller')\n",
    "    return groceries_soup\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the best hotel seller in Gold Coast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hotel_doc = get_HTML_GCHOTEL() #get the html of Gold Coast searching result\n",
    "Seller_elem= get_best_seller_soup(hotel_doc) #find the best seller of the hotel\n",
    "hotel_info=Seller_elem.parent.parent.parent.parent.parent\n",
    "hotel_name_soup = hotel_info.findAll(\"div\", {\"class\" : \"listing_title\"})[0]\n",
    "hotel_name=hotel_name_soup.a.string  #get the hotel name\n",
    "Hotel_price_soup= hotel_info.findAll(\"div\", {\"class\" : \"price-wrap\"})[0]\n",
    "hotel_price= Hotel_price_soup.div.string    #get the hotel price\n",
    "print('The best seller of hotel in Gold Coast is '+hotel_name+' and the price is '+hotel_price ) #print the best seller name and price\n",
    "\n",
    "#Please try few times if it notices failed!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the best hotel seller in Sunshine Coast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotel_doc = get_HTML_SSHOTEL() #get the html of Sunshine Coast searching result\n",
    "Seller_elem= get_best_seller_soup(hotel_doc) #find the best seller of the hotel\n",
    "hotel_info=Seller_elem.parent.parent.parent.parent.parent\n",
    "hotel_name_soup = hotel_info.findAll(\"div\", {\"class\" : \"listing_title\"})[0]\n",
    "hotel_name=hotel_name_soup.a.string  #get the hotel name\n",
    "Hotel_price_soup= hotel_info.findAll(\"div\", {\"class\" : \"price-wrap\"})[0]\n",
    "hotel_price= Hotel_price_soup.div.string    #get the hotel price\n",
    "print('The best seller of hotel in Gold Coast is '+hotel_name+' and the price is '+hotel_price ) #print the best seller name and price\n",
    "\n",
    "#Please try few times if it notices failed!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>We also want to build a partnership with the bestseller hotels, which can help us saving money if our business is stable and successful.</b> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  5. SWOT Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, All the things are done! It is time to start our business and make money now. That is very exciting!!!. But, wait! There is still a very important thing need to do. There is an old saying that knows the enemy and knows yourself, and you can fight a hundred battles with no danger of defeat. If you want to survive and become the biggest player in this industry, you must have a good understanding of yourself and your competitors. So, Let us do the SWOT Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='SWOT Analysis.png' >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<b>Strengths:</b>\n",
    "\n",
    "   Agile and Flexible: We just wanted to start our business a few weeks ago. In the beginning, our business structure and working group can change for our customer needs. The failure cost is small because our business scale is much smaller than other big players in the market.\n",
    "    \n",
    "   Knowing our target audience: This gives us direction for our marketing and ensures more consistency in our messaging, so we can build stronger relationships with customers and provide a better surfing experience for our customers. We can design special surfing activities at less cost.  \n",
    "    \n",
    "   Knowing of the best surfing spot: We have a forecast about surfing spot condition, which can help us to choose the best surfing spot according to our customers and what time is the best.\n",
    "   \n",
    "   Knowing of the best seller hotel: The best seller hotel is chosen by customers. So, the position in the market should meet most customers needs. It has a good location, services, amenities and affordable price for most customers, which can help us save the money and delivery good accommodation experience at the same time.\n",
    "    \n",
    "    \n",
    "    \n",
    "<b>Weaknesses:</b>\n",
    "\n",
    "Financial crisis: As a small surfing business organization, we do not have enough other budgets for the emergency. As a result, it is very fatal when an emergency happened if we can not get financial support from the sponsor. \n",
    "\n",
    "Partnership: In the beginning, we haven't built partnerships with other relevant stakeholders. That means we have more cost than our competitors who has a great relationship with hotels.\n",
    "\n",
    "Lack of experience: We do not have the experience of running business before. This will be a great challenge for our organization.\n",
    "\n",
    "Lack of Coaches: Our organization was just formed by some surfers. That means we do not have enough professional coaches to guide our customers and keep their safety.  \n",
    "\n",
    "\n",
    "\n",
    "<b>Opportunities:</b>\n",
    "\n",
    "More old surfers: We found there will be more and more old people like surfing in the future and our competitors haven't put their focus on this consumer group.\n",
    "\n",
    "More foreign tourists: There are more and more tourists from other countries want to experience surfing this kind of spot and they also want to spend more money to get better service. Maybe we can design a special service for foreign tourists in the future and gain money.\n",
    "\n",
    "\n",
    "<b>Threats:</b>\n",
    "\n",
    "Global warm: The climate change makes the temperature, weather and large-scale ocean circulation become very unstable. Climate change is very important for us because whether we can run our business depends on our beautiful ocean.\n",
    "\n",
    "No sponsor: Can not get enough financial support.\n",
    "\n",
    "Competitors: Other big players want to expand their market share and they have big strong strength and enough resources to use.\n",
    "\n",
    "Political: If Australia has bad relationships with other countries, it will cause the foreign tourists  decrease. \n",
    "\n",
    "\n",
    "<b>Conclusion:</b>\n",
    "\n",
    "All things start is bad, but after the first pace is stridden,  it will get easier. Our biggest problem now is the financial problem and we do not have any experience of running a business. The first important thing for us is to find a professional person who can guide us on how to run a business. Then, it is time to build relationships with partners from hospitality, restaurant and surfing. We have confidence that we will be successful in the future because we do really want to bring happiness, healthiness and excitement of surfing to our customers.\n",
    "\n",
    "So what are you waiting for? Contact us now!!! Go grab a board and catch some waves and find out for yourself what the physical, mental and emotional rewards that surfing can give you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
